{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# where you want all HF files (models, tokenizers, caches, etc.) to live:\n",
    "os.environ['HF_HOME'] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-12b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "\n",
    "# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n",
    "# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n",
    "# It has a slightly soft, natural feel, likely captured in daylight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load model & processor once at startup\n",
    "model_id = \"google/gemma-3-12b-it\"\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 2. Define your inference function\n",
    "def describe(image, user_prompt):\n",
    "    # ensure PIL.Image\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\":  user_prompt}\n",
    "        ]},\n",
    "    ]\n",
    "\n",
    "    # prepare inputs\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    # run generation\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        tokens = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    gen_tokens = tokens[0, input_len:]\n",
    "    return processor.decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 3. Build the Gradio interface with BOTH image + text inputs\n",
    "iface = gr.Interface(\n",
    "    fn=describe,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "        gr.Textbox(lines=2,\n",
    "                   placeholder=\"e.g. Describe this image in detail\",\n",
    "                   label=\"Your Prompt\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Model’s Response\"),\n",
    "    title=\"Gemma-3 Image & Text Describer\",\n",
    "    description=\"Upload an image and enter any text prompt to get a multimodal response.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load model & processor once at startup\n",
    "model_id = \"google/gemma-3-12b-it\"\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 2. Define your inference function\n",
    "def describe(image=None, user_prompt=\"\"):\n",
    "    # Reject if nothing is provided\n",
    "    if image is None and not user_prompt.strip():\n",
    "        return \"⚠️ Please upload an image or enter a text prompt (or both).\"\n",
    "\n",
    "    # System message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Build user content dynamically\n",
    "    user_contents = []\n",
    "    if image is not None:\n",
    "        # ensure PIL.Image\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        user_contents.append({\"type\": \"image\", \"image\": image})\n",
    "    if user_prompt.strip():\n",
    "        user_contents.append({\"type\": \"text\", \"text\": user_prompt.strip()})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_contents})\n",
    "\n",
    "    # Prepare inputs for the model\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    # Run generation\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        tokens = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    gen_tokens = tokens[0, input_len:]\n",
    "    return processor.decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 3. Build a Gradio interface with optional inputs\n",
    "iface = gr.Interface(\n",
    "    fn=describe,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image (optional)\", optional=True),\n",
    "        gr.Textbox(\n",
    "            lines=2,\n",
    "            placeholder=\"Enter a text prompt (optional)\",\n",
    "            label=\"Your Prompt\",\n",
    "            optional=True\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Model’s Response\"),\n",
    "    title=\"Gemma-3 Multimodal Describer\",\n",
    "    description=(\n",
    "        \"Upload an image, enter text, or both. \"\n",
    "        \"The model will respond based on whichever inputs you provide.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
