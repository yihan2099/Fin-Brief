{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# where you want all HF files (models, tokenizers, caches, etc.) to live:\n",
    "os.environ['HF_HOME'] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06bf3e5493c4258a8e06fe4c2d91fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ model loaded   (param dtype = torch.float32 )\n",
      "Prediction: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Stable inference for Gemma-3-12B-IT + AG-News LoRA\n",
    "--------------------------------------------------\n",
    "Weights   : 4-bit NF4 (base)  +  float32 (LoRA)\n",
    "Compute   : float32\n",
    "Hardware  : 3× RTX 3090 (24 GiB each)\n",
    "Returns   : 'World' | 'Sports' | 'Business' | 'Sci/Tech'\n",
    "\"\"\"\n",
    "\n",
    "# 0 ─ Environment -----------------------------------------------------------\n",
    "import os, torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"        # edit if fewer GPUs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32      = True\n",
    "\n",
    "# 1 ─ Libraries -------------------------------------------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 2 ─ Constants -------------------------------------------------------------\n",
    "BASE_ID  = \"google/gemma-3-12b-it\"\n",
    "ADAPTER  = \"gemma3-agnews-lora/adapter\"\n",
    "LABELS   = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "# 3 ─ 4-bit base model (weights fp32, compute fp32) -------------------------\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype    = torch.float32,   # compute in fp32 → no overflow\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID,\n",
    "    torch_dtype         = torch.float32,        # ← KEEP full precision weights\n",
    "    attn_implementation = \"eager\",\n",
    "    device_map          = \"auto\",\n",
    "    quantization_config = bnb_cfg,\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, ADAPTER, torch_dtype=torch.float32)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ model loaded   (param dtype =\", next(model.parameters()).dtype, \")\")\n",
    "\n",
    "# 4 ─ Tokeniser & label-IDs -----------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tok.pad_token = tok.eos_token\n",
    "LABEL_IDS = [tok(lbl, add_special_tokens=False).input_ids[0] for lbl in LABELS]\n",
    "\n",
    "# 5 ─ Classifier -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def classify_article(text: str) -> str:\n",
    "    sys = {\"role\":\"system\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":\"You are a helpful assistant. Answer with exactly one label \"\n",
    "                  \"from [World, Sports, Business, Sci/Tech].\"}]}\n",
    "    usr = {\"role\":\"user\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":f\"Classify the following news article:\\n\\n{text}\"}]}\n",
    "\n",
    "    bundle = tok.apply_chat_template(\n",
    "        [sys, usr], tokenize=True, add_generation_prompt=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    bundle = {k: v.to(next(model.parameters()).device) for k,v in bundle.items()}\n",
    "\n",
    "    # forward pass – fp32 compute, no overflow\n",
    "    logits = model(**bundle).logits[:, -1, :]          # [1, vocab]\n",
    "    probs  = logits[0, LABEL_IDS]\n",
    "    if torch.isnan(probs).any():\n",
    "        raise RuntimeError(\"NaNs persist -- adapter file is corrupt.\")\n",
    "\n",
    "    return LABELS[probs.argmax().item()]\n",
    "\n",
    "# 6 ─ Demo -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    art = (\"Nvidia’s quarterly revenue soared 265 % year-on-year \"\n",
    "           \"thanks to AI demand.\")\n",
    "    print(\"Prediction:\", classify_article(art))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Business\n"
     ]
    }
   ],
   "source": [
    "# 5 ─ Classifier -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def classify_article(text: str) -> str:\n",
    "    sys = {\"role\":\"system\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":\"You are a helpful assistant. Answer with exactly one label \"\n",
    "                  \"from [World, Sports, Business, Sci/Tech].\"}]}\n",
    "    usr = {\"role\":\"user\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":f\"Classify the following news article:\\n\\n{text}\"}]}\n",
    "\n",
    "    bundle = tok.apply_chat_template(\n",
    "        [sys, usr], tokenize=True, add_generation_prompt=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    bundle = {k: v.to(next(model.parameters()).device) for k,v in bundle.items()}\n",
    "\n",
    "    # forward pass – fp32 compute, no overflow\n",
    "    logits = model(**bundle).logits[:, -1, :]          # [1, vocab]\n",
    "    probs  = logits[0, LABEL_IDS]\n",
    "    if torch.isnan(probs).any():\n",
    "        raise RuntimeError(\"NaNs persist -- adapter file is corrupt.\")\n",
    "\n",
    "    return LABELS[probs.argmax().item()]\n",
    "\n",
    "# 6 ─ Demo -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    art = (\"Economy is booming\")\n",
    "    print(\"Prediction:\", classify_article(art))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test & debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemma‑3‑12B base …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8d7eb589fd41e987e674cf7771307c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model & LoRA adapter ready → cuda:0\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Gradio web demo for Gemma‑3‑12B‑IT + AG‑News LoRA classifier\n",
    "-----------------------------------------------------------\n",
    "Weights   : 4‑bit NF4 (base)  +  float32 (LoRA)\n",
    "Hardware  : 3× RTX 3090 (24 GiB each)\n",
    "Returns   : one of 4 labels with probabilities\n",
    "\n",
    "Run:\n",
    "    conda activate science   # or your venv\n",
    "    pip install gradio transformers peft bitsandbytes datasets accelerate torch>=2.2\n",
    "    CUDA_VISIBLE_DEVICES=1,2,3 python gemma_agnews_gradio.py\n",
    "\"\"\"\n",
    "\n",
    "# 0 ─ Environment -----------------------------------------------------------\n",
    "import os, torch, gradio as gr\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")          # edit if needed\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32      = True\n",
    "\n",
    "# 1 ─ Libraries -------------------------------------------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 2 ─ Constants -------------------------------------------------------------\n",
    "BASE_ID  = \"google/gemma-3-12b-it\"\n",
    "ADAPTER  = \"gemma3-agnews-lora/adapter\"\n",
    "LABELS   = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "# 3 ─ 4‑bit base model (weights fp32, compute fp32) -------------------------\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype    = torch.float32,\n",
    ")\n",
    "\n",
    "print(\"Loading Gemma‑3‑12B base …\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID,\n",
    "    torch_dtype         = torch.float32,\n",
    "    attn_implementation = \"eager\",\n",
    "    device_map          = \"auto\",\n",
    "    quantization_config = bnb_cfg,\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER, torch_dtype=torch.float32)\n",
    "model.eval()\n",
    "print(\"✓ Model & LoRA adapter ready →\", next(model.parameters()).device)\n",
    "\n",
    "# 4 ─ Tokeniser & label‑IDs --------------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tok.pad_token = tok.eos_token\n",
    "LABEL_IDS = [tok(lbl, add_special_tokens=False).input_ids[0] for lbl in LABELS]\n",
    "\n",
    "# 5 ─ Classifier -------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def classify_article(text: str):\n",
    "    \"\"\"Return a dict {label: probability}. Compatible with gradio.Label.\"\"\"\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return \"Please paste a news article ≥ 10 characters.\"\n",
    "\n",
    "    sys = {\"role\":\"system\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":\"You are a helpful assistant. Answer with exactly one label \"\n",
    "                  \"from [World, Sports, Business, Sci/Tech].\"}]}\n",
    "    usr = {\"role\":\"user\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":f\"Classify the following news article:\\n\\n{text}\"}]}\n",
    "\n",
    "    bundle = tok.apply_chat_template(\n",
    "        [sys, usr], tokenize=True, add_generation_prompt=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    bundle = {k: v.to(next(model.parameters()).device) for k,v in bundle.items()}\n",
    "\n",
    "    logits = model(**bundle).logits[:, -1, :]          # [1, vocab]\n",
    "    probs  = torch.softmax(logits[0, LABEL_IDS], dim=-1)\n",
    "\n",
    "    # Build dict for gradio.Label\n",
    "    return {lbl: float(p) for lbl, p in zip(LABELS, probs)}\n",
    "\n",
    "# 6 ─ Gradio Interface -------------------------------------------------------\n",
    "demo = gr.Interface(\n",
    "    fn          = classify_article,\n",
    "    inputs      = gr.Textbox(lines=12, label=\"Paste a news article\"),\n",
    "    outputs     = gr.Label(num_top_classes=4, label=\"Predicted class\"),\n",
    "    title       = \"Gemma‑3 News Classifier\",\n",
    "    description = (\n",
    "        \"A lightweight 4‑bit Gemma‑3‑12B model fine‑tuned via QLoRA on \\n\"\n",
    "        \"AG‑News (500‑sample demo). Paste any English news article to get the\\n\"\n",
    "        \"predicted category (World / Sports / Business / Sci/Tech).\"\n",
    "    ),\n",
    "    examples=[\n",
    "        [\"Nvidia’s quarterly revenue soared 265 % year‑on‑year thanks to AI demand.\"],\n",
    "        [\"Real Madrid lifted their 15th Champions League trophy after a 2‑0 win.\"],\n",
    "    ],\n",
    "    cache_examples=False,\n",
    ")\n",
    "\n",
    "# 7 ─ Launch -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
