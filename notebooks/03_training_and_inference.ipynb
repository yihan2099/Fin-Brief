{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# where you want all HF files (models, tokenizers, caches, etc.) to live:\n",
    "os.environ['HF_HOME'] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacebeedb7dd42188c29aa89ef856e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ model loaded   (param dtype = torch.float32 )\n",
      "Prediction: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Stable inference for Gemma-3-12B-IT + AG-News LoRA\n",
    "--------------------------------------------------\n",
    "Weights   : 4-bit NF4 (base)  +  float32 (LoRA)\n",
    "Compute   : float32\n",
    "Hardware  : 3× RTX 3090 (24 GiB each)\n",
    "Returns   : 'World' | 'Sports' | 'Business' | 'Sci/Tech'\n",
    "\"\"\"\n",
    "\n",
    "# 0 ─ Environment -----------------------------------------------------------\n",
    "import os, torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"        # edit if fewer GPUs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32      = True\n",
    "\n",
    "# 1 ─ Libraries -------------------------------------------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 2 ─ Constants -------------------------------------------------------------\n",
    "BASE_ID  = \"google/gemma-3-12b-it\"\n",
    "ADAPTER  = \"gemma3-agnews-lora/adapter\"\n",
    "LABELS   = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "# 3 ─ 4-bit base model (weights fp32, compute fp32) -------------------------\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype    = torch.float32,   # compute in fp32 → no overflow\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID,\n",
    "    torch_dtype         = torch.float32,        # ← KEEP full precision weights\n",
    "    attn_implementation = \"eager\",\n",
    "    device_map          = \"auto\",\n",
    "    quantization_config = bnb_cfg,\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, ADAPTER, torch_dtype=torch.float32)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ model loaded   (param dtype =\", next(model.parameters()).dtype, \")\")\n",
    "\n",
    "# 4 ─ Tokeniser & label-IDs -----------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tok.pad_token = tok.eos_token\n",
    "LABEL_IDS = [tok(lbl, add_special_tokens=False).input_ids[0] for lbl in LABELS]\n",
    "\n",
    "# 5 ─ Classifier -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def classify_article(text: str) -> str:\n",
    "    sys = {\"role\":\"system\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":\"You are a helpful assistant. Answer with exactly one label \"\n",
    "                  \"from [World, Sports, Business, Sci/Tech].\"}]}\n",
    "    usr = {\"role\":\"user\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":f\"Classify the following news article:\\n\\n{text}\"}]}\n",
    "\n",
    "    bundle = tok.apply_chat_template(\n",
    "        [sys, usr], tokenize=True, add_generation_prompt=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    bundle = {k: v.to(next(model.parameters()).device) for k,v in bundle.items()}\n",
    "\n",
    "    # forward pass – fp32 compute, no overflow\n",
    "    logits = model(**bundle).logits[:, -1, :]          # [1, vocab]\n",
    "    probs  = logits[0, LABEL_IDS]\n",
    "    if torch.isnan(probs).any():\n",
    "        raise RuntimeError(\"NaNs persist -- adapter file is corrupt.\")\n",
    "\n",
    "    return LABELS[probs.argmax().item()]\n",
    "\n",
    "# 6 ─ Demo -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    art = (\"Nvidia’s quarterly revenue soared 265 % year-on-year \"\n",
    "           \"thanks to AI demand.\")\n",
    "    print(\"Prediction:\", classify_article(art))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Business\n"
     ]
    }
   ],
   "source": [
    "# 5 ─ Classifier -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def classify_article(text: str) -> str:\n",
    "    sys = {\"role\":\"system\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":\"You are a helpful assistant. Answer with exactly one label \"\n",
    "                  \"from [World, Sports, Business, Sci/Tech].\"}]}\n",
    "    usr = {\"role\":\"user\",\"content\":[{\"type\":\"text\",\n",
    "           \"text\":f\"Classify the following news article:\\n\\n{text}\"}]}\n",
    "\n",
    "    bundle = tok.apply_chat_template(\n",
    "        [sys, usr], tokenize=True, add_generation_prompt=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    bundle = {k: v.to(next(model.parameters()).device) for k,v in bundle.items()}\n",
    "\n",
    "    # forward pass – fp32 compute, no overflow\n",
    "    logits = model(**bundle).logits[:, -1, :]          # [1, vocab]\n",
    "    probs  = logits[0, LABEL_IDS]\n",
    "    if torch.isnan(probs).any():\n",
    "        raise RuntimeError(\"NaNs persist -- adapter file is corrupt.\")\n",
    "\n",
    "    return LABELS[probs.argmax().item()]\n",
    "\n",
    "# 6 ─ Demo -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    art = (\"Economy is booming\")\n",
    "    print(\"Prediction:\", classify_article(art))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test & debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
