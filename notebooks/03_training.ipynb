{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# where you want all HF files (models, tokenizers, caches, etc.) to live:\n",
    "os.environ['HF_HOME'] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Finetune Gemma-3-12B-IT on AG-News with QLoRA, TRL ≥ 0.15, 4 × RTX 3090\n",
    "\"\"\"\n",
    "\n",
    "# ────────── 1. imports ──────────────────────────────────────────────\n",
    "import os, torch, random, gc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# ────────── 2. constants ────────────────────────────────────────────\n",
    "MODEL_ID  = \"google/gemma-3-12b-it\"\n",
    "OUT_DIR   = \"gemma3-agnews-lora\"\n",
    "RAND_SEED = 42\n",
    "MAX_LEN   = 512                 # truncate articles\n",
    "LABELS    = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"      # four 3090s\n",
    "torch.manual_seed(RAND_SEED)\n",
    "\n",
    "# ────────── 3. load 4-bit base model on 4 GPUs ─────────────────────\n",
    "max_memory = {i: \"23GiB\" for i in range(torch.cuda.device_count())}\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map          = \"balanced_low_0\",   # balanced across 4 GPUs\n",
    "    max_memory          = max_memory,\n",
    "    quantization_config = bnb_cfg,\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()          # more VRAM savings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ────────── 4. attach LoRA adapter ─────────────────────────────────-\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\n",
    "                    \"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# ────────── 5. prepare dataset ─────────────────────────────────────\n",
    "def to_chatml(sample):\n",
    "    user = (f\"Classify the following news article into one of \"\n",
    "            f\"[World, Sports, Business, Sci/Tech]:\\n{sample['text']}\")\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"user\",      \"content\": user},\n",
    "        {\"role\": \"assistant\", \"content\": LABELS[int(sample['label'])]},\n",
    "    ]}\n",
    "\n",
    "ds = load_dataset(\"fancyzhx/ag_news\", split=\"train\").shuffle(seed=RAND_SEED)\n",
    "ds = ds.select(range(40_000))                        # quick demo size\n",
    "train_ds = ds.map(to_chatml, remove_columns=ds.column_names)\n",
    "\n",
    "# ────────── 6. SFTConfig (supersedes TrainingArguments) ────────────\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir                 = OUT_DIR,\n",
    "    max_length                 = MAX_LEN,\n",
    "    per_device_train_batch_size= 1,      # 1×3090 ≈ 5.8 GiB\n",
    "    gradient_accumulation_steps= 16,     # effective batch = 16\n",
    "    num_train_epochs           = 2,\n",
    "    learning_rate              = 2e-4,\n",
    "    bf16                       = True,\n",
    "    optim                      = \"paged_adamw_8bit\",\n",
    "    warmup_ratio               = 0.05,\n",
    "    lr_scheduler_type          = \"cosine\",\n",
    "    logging_steps              = 50,\n",
    "    save_total_limit           = 2,\n",
    "    report_to                  = \"tensorboard\",\n",
    "    packing                    = True,\n",
    "    seed                       = RAND_SEED,\n",
    ")\n",
    "\n",
    "# ────────── 7. launch training ─────────────────────────────────────\n",
    "trainer = SFTTrainer(model=model,\n",
    "                     train_dataset=train_ds,\n",
    "                     args=sft_cfg)\n",
    "trainer.train()\n",
    "\n",
    "# ────────── 8. save adapter ────────────────────────────────────────\n",
    "model.save_pretrained(f\"{OUT_DIR}-adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUT_DIR}-adapter\")\n",
    "print(f\"\\n✓ Adapter written to → {OUT_DIR}-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_id  = \"google/gemma-3-12b-it\"\n",
    "adapter  = \"gemma3-agnews-lora-adapter\"\n",
    "\n",
    "tok  = AutoTokenizer.from_pretrained(base_id)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_id, load_in_4bit=True, device_map=\"auto\", trust_remote_code=True)\n",
    "mdl  = PeftModel.from_pretrained(base, adapter).eval()\n",
    "\n",
    "prompt = (\n",
    "    \"Classify the following news article into one of \"\n",
    "    \"[World, Sports, Business, Sci/Tech]:\\n\"\n",
    "    \"Nvidia’s quarterly revenue soared 265 % year-on-year thanks to AI demand.\"\n",
    ")\n",
    "ids = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "out = mdl.generate(**ids, max_new_tokens=10)\n",
    "print(tok.decode(out[0][ids.input_ids.shape[1]:], skip_special_tokens=True))\n",
    "# → \"Business\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
